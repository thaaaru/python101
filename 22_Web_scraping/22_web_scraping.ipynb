{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee499ace-5701-4e35-a60a-2d11902b88ea",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "  <h1>022 - Web Scraping </h1>\n",
    "<sub>Author  :   Tharaka Mahabage   <br> <small> First Edition: April, 2024</small>\n",
    "</sub>\n",
    "</div>\n",
    "\n",
    "[<<_21](../21_Classes_and_objects/21_classes_and_objects.ipynb) | [Day 23 >>](../23_Virtual_environment/23_virtual_environment.ipynb)\n",
    "\n",
    "\n",
    "\n",
    "- [ðŸ“˜_22](#-day-22)\n",
    "  - [Python Web Scraping](#python-web-scraping)\n",
    "    - [What is Web Scrapping](#what-is-web-scrapping)\n",
    "  - [ðŸ’» Exercises:_22](#-exercises-day-22)\n",
    "\n",
    "\n",
    "## Python Web Scraping\n",
    "\n",
    "### What is Web Scrapping\n",
    "\n",
    "The internet is full of huge amount of data which can be used for different purposes. To collect this data we need to know how to scrape data from a website.\n",
    "\n",
    "Web scraping is the process of extracting and collecting data from websites and storing it on a local machine or in a database.\n",
    "\n",
    "In this section, we will use beautifulsoup and requests package to scrape data. The package version we are using is beautifulsoup 4.\n",
    "\n",
    "To start scraping websites you need _requests_, _beautifoulSoup4_ and a _website_.\n",
    "\n",
    "```sh\n",
    "pip install requests\n",
    "pip install beautifulsoup4\n",
    "```\n",
    "\n",
    "To scrape data from websites, basic understanding of HTML tags and CSS selectors is needed. We target content from a website using HTML tags, classes or/and ids.\n",
    "Let us import the requests and BeautifulSoup module\n",
    "\n",
    "```py\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "```\n",
    "\n",
    "Let us declare url variable for the website which we are going to scrape.\n",
    "\n",
    "```py\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "url = 'https://archive.ics.uci.edu/ml/datasets.php'\n",
    "\n",
    "# Lets use the requests get method to fetch the data from url\n",
    "\n",
    "response = requests.get(url)\n",
    "# lets check the status\n",
    "status = response.status_code\n",
    "print(status) # 200 means the fetching was successful\n",
    "```\n",
    "\n",
    "```sh\n",
    "200\n",
    "```\n",
    "\n",
    "Using beautifulSoup to parse content from the page\n",
    "\n",
    "```py\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "url = 'https://archive.ics.uci.edu/ml/datasets.php'\n",
    "\n",
    "response = requests.get(url)\n",
    "content = response.content # we get all the content from the website\n",
    "soup = BeautifulSoup(content, 'html.parser') # beautiful soup will give a chance to parse\n",
    "print(soup.title) # <title>UCI Machine Learning Repository: Data Sets</title>\n",
    "print(soup.title.get_text()) # UCI Machine Learning Repository: Data Sets\n",
    "print(soup.body) # gives the whole page on the website\n",
    "print(response.status_code)\n",
    "\n",
    "tables = soup.find_all('table', {'cellpadding':'3'})\n",
    "# We are targeting the table with cellpadding attribute with the value of 3\n",
    "# We can select using id, class or HTML tag , for more information check the beautifulsoup doc\n",
    "table = tables[0] # the result is a list, we are taking out data from it\n",
    "for td in table.find('tr').find_all('td'):\n",
    "    print(td.text)\n",
    "```\n",
    "\n",
    "If you run this code, you can see that the extraction is half done. You can continue doing it because it is part of exercise 1.\n",
    "For reference check the [beautifulsoup documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#quick-start)\n",
    "\n",
    "ðŸŒ• You are so special, you are progressing everyday. You are left with only eight days to your way to greatness. Now do some exercises for your brain and muscles.\n",
    "\n",
    "## ðŸ’» Exercises:_22\n",
    "\n",
    "1. Scrape the following website and store the data as json file(url = 'http://www.bu.edu/president/boston-university-facts-stats/').\n",
    "1. Extract the table in this url (https://archive.ics.uci.edu/ml/datasets.php) and change it to a json file\n",
    "2. Scrape the presidents table and store the data as json(https://en.wikipedia.org/wiki/List_of_presidents_of_the_United_States). The table is not very structured and the scrapping may take very long time.\n",
    "\n",
    "ðŸŽ‰ CONGRATULATIONS ! ðŸŽ‰\n",
    "\n",
    "[<<_21](../21_Web_scraping/21_class_and_object.ipynb) | [Day 23 >>](../23_Virtual_environment/23_virtual_environment.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4e31a9-a4e9-4ceb-aff5-e1e0c7398c69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
